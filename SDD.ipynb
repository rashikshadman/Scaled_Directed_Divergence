{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os, json\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from flashtorch.utils import apply_transforms, load_image \n",
    "import net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fc6217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_transform(pil_image):\n",
    "    \"\"\"Custom transform incorporating face alignment and preprocessing.\"\"\"\n",
    "    #aligned_rgb_img = align.get_aligned_face(pil_image)\n",
    "    np_img = np.array(pil_image)\n",
    "    bgr_img = ((np_img[:, :, ::-1] / 255.0) - 0.5) / 0.5  # Normalize to [-1, 1]\n",
    "    tensor = torch.tensor(bgr_img.transpose(2, 0, 1)).float()\n",
    "    return tensor\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bac835",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_input_tensors(img):\n",
    "    \n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.Lambda(lambda img: custom_transform(img)),\n",
    "    # Add more transformations if needed (e.g., data augmentation)\n",
    "    ])\n",
    "    \n",
    "    # unsqeeze converts single image to batch of 1\n",
    "    return transform(img).unsqueeze(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9efa96",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_input_tensors_no_resize(img):\n",
    "    \n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "    #transforms.Resize((112, 112)),\n",
    "    transforms.Lambda(lambda img: custom_transform(img)),\n",
    "    # Add more transformations if needed (e.g., data augmentation)\n",
    "    ])\n",
    "    \n",
    "    # unsqeeze converts single image to batch of 1\n",
    "    return transform(img).unsqueeze(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c85419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the path to pretrained AdaFace models\n",
    "adaface_models = {\n",
    "    'ir_101': \"models/adaface_ir101_webface12m.ckpt\",\n",
    "}\n",
    "\n",
    "def load_pretrained_model(architecture='ir_101'):\n",
    "    \"\"\"Load the pretrained AdaFace model.\"\"\"\n",
    "    assert architecture in adaface_models.keys(), f\"Architecture {architecture} not supported.\"\n",
    "    model = net.build_model(architecture)\n",
    "    statedict = torch.load(adaface_models[architecture])['state_dict']\n",
    "    # Remove 'model.' prefix from keys\n",
    "    model_statedict = {key[6:]: val for key, val in statedict.items() if key.startswith('model.')}\n",
    "    model.load_state_dict(model_statedict)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    return model\n",
    "    \n",
    "    \n",
    "class FaceClassifier(nn.Module):\n",
    "    def __init__(self, feature_dim, num_classes, freeze_feature_extractor=True):\n",
    "        super(FaceClassifier, self).__init__()\n",
    "        self.feature_extractor = load_pretrained_model('ir_101')\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.classifier = nn.Linear(self.feature_dim, self.num_classes)\n",
    "\n",
    "        if freeze_feature_extractor:\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = False  # Freeze feature extractor\n",
    "        #else:\n",
    "             # Unfreeze the feature extractor\n",
    "            #for param in self.feature_extractor.parameters():\n",
    "                #param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features, _ = self.feature_extractor(x)\n",
    "        #print(features.shape)\n",
    "        out = self.classifier(features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba87c497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(path):\n",
    "    with open(os.path.abspath(path), 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB') \n",
    "        \n",
    "img = load_image('image_path')\n",
    "\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70836ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check for cuda and set gpu or cpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "#print(device)\n",
    "\n",
    "\n",
    "face_model = 'models/facescrub_adamW_webface.pt'\n",
    "model_f = torch.load(face_model)\n",
    "model_f.to(device)\n",
    "\n",
    "img_t = get_input_tensors(img)\n",
    "img_t_ = img_t.cuda()\n",
    "model_f.eval()\n",
    "#print(model_f)\n",
    "\n",
    "logits = model_f(img_t.cuda())\n",
    "#print(img_t.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b055c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax(logits, dim=1)\n",
    "probs5 = probs.topk(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2281d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f445be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['subject_1', 'subject_10', 'subject_100', 'subject_101', 'subject_102', 'subject_103', 'subject_104', 'subject_105', 'subject_106', 'subject_107', 'subject_108', 'subject_109', 'subject_11', 'subject_110', 'subject_111', 'subject_112', 'subject_113', 'subject_114', 'subject_115', 'subject_116', 'subject_117', 'subject_118', 'subject_119', 'subject_12', 'subject_120', 'subject_121', 'subject_122', 'subject_123', 'subject_124', 'subject_125', 'subject_126', 'subject_127', 'subject_128', 'subject_129', 'subject_13', 'subject_130', 'subject_131', 'subject_132', 'subject_133', 'subject_134', 'subject_135', 'subject_136', 'subject_137', 'subject_138', 'subject_139', 'subject_14', 'subject_140', 'subject_141', 'subject_142', 'subject_143', 'subject_144', 'subject_145', 'subject_146', 'subject_147', 'subject_148', 'subject_149', 'subject_15', 'subject_150', 'subject_151', 'subject_152', 'subject_153', 'subject_154', 'subject_155', 'subject_156', 'subject_157', 'subject_158', 'subject_159', 'subject_16', 'subject_160', 'subject_161', 'subject_162', 'subject_163', 'subject_164', 'subject_165', 'subject_166', 'subject_167', 'subject_168', 'subject_169', 'subject_17', 'subject_170', 'subject_171', 'subject_172', 'subject_173', 'subject_174', 'subject_175', 'subject_176', 'subject_177', 'subject_178', 'subject_179', 'subject_18', 'subject_180', 'subject_181', 'subject_182', 'subject_183', 'subject_184', 'subject_185', 'subject_186', 'subject_187', 'subject_188', 'subject_189', 'subject_19', 'subject_190', 'subject_191', 'subject_192', 'subject_193', 'subject_194', 'subject_195', 'subject_196', 'subject_197', 'subject_198', 'subject_199', 'subject_2', 'subject_20', 'subject_200', 'subject_201', 'subject_202', 'subject_203', 'subject_204', 'subject_205', 'subject_206', 'subject_207', 'subject_208', 'subject_209', 'subject_21', 'subject_210', 'subject_211', 'subject_212', 'subject_213', 'subject_214', 'subject_215', 'subject_216', 'subject_217', 'subject_218', 'subject_219', 'subject_22', 'subject_220', 'subject_221', 'subject_222', 'subject_223', 'subject_224', 'subject_225', 'subject_226', 'subject_227', 'subject_228', 'subject_229', 'subject_23', 'subject_230', 'subject_231', 'subject_232', 'subject_233', 'subject_234', 'subject_235', 'subject_236', 'subject_237', 'subject_238', 'subject_239', 'subject_24', 'subject_240', 'subject_241', 'subject_242', 'subject_243', 'subject_244', 'subject_245', 'subject_246', 'subject_247', 'subject_248', 'subject_249', 'subject_25', 'subject_250', 'subject_251', 'subject_252', 'subject_253', 'subject_254', 'subject_255', 'subject_256', 'subject_257', 'subject_258', 'subject_259', 'subject_26', 'subject_260', 'subject_261', 'subject_262', 'subject_263', 'subject_264', 'subject_265', 'subject_266', 'subject_267', 'subject_268', 'subject_269', 'subject_27', 'subject_270', 'subject_271', 'subject_272', 'subject_273', 'subject_274', 'subject_275', 'subject_276', 'subject_277', 'subject_278', 'subject_279', 'subject_28', 'subject_280', 'subject_281', 'subject_282', 'subject_283', 'subject_284', 'subject_285', 'subject_286', 'subject_287', 'subject_288', 'subject_289', 'subject_29', 'subject_290', 'subject_291', 'subject_292', 'subject_293', 'subject_294', 'subject_295', 'subject_296', 'subject_297', 'subject_298', 'subject_299', 'subject_3', 'subject_30', 'subject_300', 'subject_301', 'subject_302', 'subject_303', 'subject_304', 'subject_305', 'subject_306', 'subject_307', 'subject_308', 'subject_309', 'subject_31', 'subject_310', 'subject_311', 'subject_312', 'subject_313', 'subject_314', 'subject_315', 'subject_316', 'subject_317', 'subject_318', 'subject_319', 'subject_32', 'subject_320', 'subject_321', 'subject_322', 'subject_323', 'subject_324', 'subject_325', 'subject_326', 'subject_327', 'subject_328', 'subject_329', 'subject_33', 'subject_330', 'subject_331', 'subject_332', 'subject_333', 'subject_334', 'subject_335', 'subject_336', 'subject_337', 'subject_338', 'subject_339', 'subject_34', 'subject_340', 'subject_341', 'subject_342', 'subject_343', 'subject_344', 'subject_345', 'subject_346', 'subject_347', 'subject_348', 'subject_349', 'subject_35', 'subject_350', 'subject_351', 'subject_352', 'subject_353', 'subject_354', 'subject_355', 'subject_356', 'subject_357', 'subject_358', 'subject_359', 'subject_36', 'subject_360', 'subject_361', 'subject_362', 'subject_363', 'subject_364', 'subject_365', 'subject_366', 'subject_367', 'subject_368', 'subject_369', 'subject_37', 'subject_370', 'subject_371', 'subject_372', 'subject_373', 'subject_374', 'subject_375', 'subject_376', 'subject_377', 'subject_378', 'subject_379', 'subject_38', 'subject_380', 'subject_381', 'subject_382', 'subject_383', 'subject_384', 'subject_385', 'subject_386', 'subject_387', 'subject_388', 'subject_389', 'subject_39', 'subject_390', 'subject_391', 'subject_392', 'subject_393', 'subject_394', 'subject_395', 'subject_396', 'subject_397', 'subject_398', 'subject_399', 'subject_4', 'subject_40', 'subject_400', 'subject_401', 'subject_402', 'subject_403', 'subject_404', 'subject_405', 'subject_406', 'subject_407', 'subject_408', 'subject_409', 'subject_41', 'subject_410', 'subject_411', 'subject_412', 'subject_413', 'subject_414', 'subject_415', 'subject_416', 'subject_417', 'subject_418', 'subject_419', 'subject_42', 'subject_420', 'subject_421', 'subject_422', 'subject_423', 'subject_424', 'subject_425', 'subject_426', 'subject_427', 'subject_428', 'subject_429', 'subject_43', 'subject_430', 'subject_431', 'subject_432', 'subject_433', 'subject_434', 'subject_435', 'subject_436', 'subject_437', 'subject_438', 'subject_439', 'subject_44', 'subject_440', 'subject_441', 'subject_442', 'subject_443', 'subject_444', 'subject_445', 'subject_446', 'subject_447', 'subject_448', 'subject_449', 'subject_45', 'subject_450', 'subject_451', 'subject_452', 'subject_453', 'subject_454', 'subject_455', 'subject_456', 'subject_457', 'subject_458', 'subject_459', 'subject_46', 'subject_460', 'subject_461', 'subject_462', 'subject_463', 'subject_464', 'subject_465', 'subject_466', 'subject_467', 'subject_468', 'subject_469', 'subject_47', 'subject_470', 'subject_471', 'subject_472', 'subject_473', 'subject_474', 'subject_475', 'subject_476', 'subject_477', 'subject_478', 'subject_479', 'subject_48', 'subject_480', 'subject_481', 'subject_482', 'subject_483', 'subject_484', 'subject_485', 'subject_486', 'subject_487', 'subject_488', 'subject_489', 'subject_49', 'subject_490', 'subject_491', 'subject_492', 'subject_493', 'subject_494', 'subject_495', 'subject_496', 'subject_497', 'subject_498', 'subject_499', 'subject_5', 'subject_50', 'subject_500', 'subject_501', 'subject_502', 'subject_503', 'subject_504', 'subject_505', 'subject_506', 'subject_507', 'subject_508', 'subject_509', 'subject_51', 'subject_510', 'subject_511', 'subject_512', 'subject_513', 'subject_514', 'subject_515', 'subject_516', 'subject_517', 'subject_518', 'subject_519', 'subject_52', 'subject_520', 'subject_521', 'subject_522', 'subject_523', 'subject_524', 'subject_525', 'subject_526', 'subject_527', 'subject_528', 'subject_529', 'subject_53', 'subject_530', 'subject_54', 'subject_55', 'subject_56', 'subject_57', 'subject_58', 'subject_59', 'subject_6', 'subject_60', 'subject_61', 'subject_62', 'subject_63', 'subject_64', 'subject_65', 'subject_66', 'subject_67', 'subject_68', 'subject_69', 'subject_7', 'subject_70', 'subject_71', 'subject_72', 'subject_73', 'subject_74', 'subject_75', 'subject_76', 'subject_77', 'subject_78', 'subject_79', 'subject_8', 'subject_80', 'subject_81', 'subject_82', 'subject_83', 'subject_84', 'subject_85', 'subject_86', 'subject_87', 'subject_88', 'subject_89', 'subject_9', 'subject_90', 'subject_91', 'subject_92', 'subject_93', 'subject_94', 'subject_95', 'subject_96', 'subject_97', 'subject_98', 'subject_99']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697dbfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import io \n",
    "\n",
    "import scipy.ndimage as nd \n",
    "\n",
    "from flashtorch.saliency import Backprop \n",
    "from flashtorch.activmax import GradientAscent \n",
    "\n",
    "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2638a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Hook():\n",
    "    def __init__(self, module, backward=False):\n",
    "        if backward==False:\n",
    "            self.hook = module.register_forward_hook(self.hook_fn)\n",
    "        else:\n",
    "            self.hook = module.register_backward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "    def close(self):\n",
    "        self.hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2575528",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_class_cam(model, _, activations, class_index):\n",
    "    \n",
    "    out_features = activations.output.squeeze(0)\n",
    "    out_features = np.transpose(out_features.cpu().detach(), (1,2,0))\n",
    "    fw = model.classifier.weight[class_index,:]\n",
    "    cam = np.dot(out_features.detach(), fw.detach().cpu())\n",
    "    return cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd4d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_score_cam(model, input_tensor, target_class, activations):\n",
    "    \"\"\"\n",
    "    Generate Score-CAM for a specific class.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The classification model.\n",
    "        input_tensor (torch.Tensor): Input image tensor of shape (1, C, H, W).\n",
    "        target_class (int): The target class index.\n",
    "        activations: Hook object containing feature maps from the target layer.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Score-CAM heatmap.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Get activation maps from hook (shape: [C, H, W])\n",
    "        act = activations.output.squeeze(0)  # Remove batch dim\n",
    "        num_channels, h, w = act.shape\n",
    "\n",
    "        # Normalize activation maps to [0, 1]\n",
    "        min_vals = torch.amin(act, dim=(1, 2), keepdim=True)\n",
    "        max_vals = torch.amax(act, dim=(1, 2), keepdim=True)\n",
    "        act = (act - min_vals) / (max_vals - min_vals + 1e-8)\n",
    "\n",
    "        # Resize each activation map to input size (H, W)\n",
    "        _, _, H, W = input_tensor.shape\n",
    "        upsampled_acts = F.interpolate(act.unsqueeze(0), size=(H, W), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        weights = []\n",
    "        for i in range(num_channels):\n",
    "            masked_input = input_tensor * upsampled_acts[0, i:i+1, :, :]\n",
    "            output = model(masked_input)\n",
    "            score = F.softmax(output, dim=1)[0, target_class].item()\n",
    "            weights.append(score)\n",
    "        \n",
    "        weights = np.array(weights).reshape(-1, 1, 1)\n",
    "        cam = (act.cpu().numpy() * weights).sum(axis=0)\n",
    "\n",
    "        cam = np.maximum(cam, 0)  # ReLU\n",
    "        #cam = cam - np.min(cam)\n",
    "        #cam = cam / (np.max(cam) + 1e-8)  # Normalize to [0, 1]\n",
    "        cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam) + 1e-8)\n",
    "        return cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8436b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sub_plots_op(original, activations, pclasses):\n",
    "    n = len(activations)\n",
    "    f, axarr = plt.subplots(1, n, figsize=(n * 5, 5))\n",
    "    \n",
    "    # Reverse the custom transformation to display the original image properly\n",
    "    inp = torch.squeeze(original, 0).numpy().transpose((1, 2, 0))  # Change to (H, W, C)\n",
    "    inp = ((inp * 0.5) + 0.5) * 255  # De-normalize from [-1, 1] to [0, 255]\n",
    "    inp = inp[:, :, ::-1]  # Convert back from BGR to RGB\n",
    "    inp = np.clip(inp / 255.0, 0, 1)  # Normalize to [0, 1] for display\n",
    "    \n",
    "    # Display each activation map with the original image as background\n",
    "    for i, cam in enumerate(activations):\n",
    "        axarr[i].axis('off')\n",
    "        axarr[i].imshow(inp, cmap='jet', alpha=1)\n",
    "        axarr[i].set_title(pclasses[i])\n",
    "        \n",
    "        # Calculate zoom factor based on the original and cam dimensions\n",
    "        orig_h, orig_w = inp.shape[:2]\n",
    "        \n",
    "        print(\"image_shape:\",inp.shape[:2])\n",
    "        print(\"cam_shape:\",cam.shape)\n",
    "        \n",
    "        cam_h, cam_w = cam.shape\n",
    "        zoom_factor = (orig_h / cam_h, orig_w / cam_w)\n",
    "        \n",
    "        # Rescale the activation map\n",
    "        zcam = nd.zoom(cam, zoom=zoom_factor, order=1)\n",
    "        \n",
    "        #heatmap = cv2.resize(cam, (inp.shape[1], inp.shape[0])) \n",
    "        axarr[i].imshow(zcam, cmap='jet', alpha=0.4)\n",
    "    \n",
    "    # Add a short pause for the plot to render\n",
    "    plt.pause(0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be751c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "SDD_Model = 'models/facescrub_adamW_webface.pt'\n",
    "#COVID_MODEL = 'models/xrayModel.pt'\n",
    "# COVID_MODEL = 'models/resnetCovidNet.pt'\n",
    "model = torch.load(SDD_Model)\n",
    "#model = torchvision.models.resnet152(pretrained=True)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "  \n",
    "#target_class = 2\n",
    "   \n",
    "\n",
    "hook = Hook(model.feature_extractor.body[48].res_layer[4])\n",
    "output = model(img_t.cuda())\n",
    "\n",
    "cams = []\n",
    "pclasses = []\n",
    "\n",
    "# Iterate over the top 5 classes and generate CAM for each\n",
    "for i in range(5):\n",
    "    class_index = probs5.indices[0, i].item()  # Get the class index for the top-i prediction\n",
    "    #cam = get_score_cam(model, img_t_, class_index, hook)\n",
    "    cam = get_class_cam(model, output, hook, class_index)  # Generate CAM for this class\n",
    "    #cam = np.maximum(cam, 0)\n",
    "    #cam_norm = cam / cam.max()\n",
    "    cams.append(cam)\n",
    "    \n",
    "    # Append the corresponding class name from 'class_names'\n",
    "    pclasses.append(class_names[class_index])\n",
    "\n",
    "# Now 'cams' contains the CAMs for the top 5 classes,\n",
    "# and 'pclasses' contains the corresponding subject names.\n",
    "    \n",
    "\n",
    "   \n",
    "print(\"CAM 0: \\n\", cams[0])\n",
    "print(cams[0].shape)\n",
    "print('CAM 1: \\n', cams[1])\n",
    "print('CAM 2: \\n', cams[2])\n",
    "#scale = 5 * (np.mean([cam.mean() for cam in cams[1:]]) / cams[0].mean())\n",
    "scale = 5 * (np.mean([np.abs(cam).mean() for cam in cams[1:]]) / np.abs(cams[0]).mean())\n",
    "scale = np.clip(scale, 5, 50)\n",
    "print(\"Scale:\", scale)\n",
    "cams[0] = cams[0] * scale\n",
    "diff = cams[0] - cams[1]\n",
    "diff = diff - cams[2]\n",
    "diff = diff - cams[3]\n",
    "diff = diff - cams[4]\n",
    "#diff = np.exp(0.2 * diff)\n",
    "# Compute differences\n",
    "#diff = cams[0] - np.mean(cams[1:], axis=0)\n",
    "\n",
    "# Compute dynamic exponential factor\n",
    "std_dev = np.std(diff)\n",
    "factor = 0.2 + (std_dev / (std_dev + 1)) * 0.2  # Adaptive factor based on spread\n",
    "\n",
    "# Apply exponential\n",
    "diff = np.exp(factor * diff)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cams.append(diff)\n",
    "\n",
    "pclasses.append(\"SDD_\" + pclasses[0])\n",
    "\n",
    "original = img_t.detach().cpu()\n",
    "show_sub_plots_op(original, cams, pclasses)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "    \n",
    "pclass = \"SDD_CAM\"\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441da275",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def apply_mask(image, heatmap, mask_type='black', threshold=np.percentile(diff, 80)):\n",
    " \n",
    "     \n",
    " \n",
    "    image = np.array(image)\n",
    "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))  # Resize heatmap to image size\n",
    "    mask = heatmap > threshold  # Create a binary mask of important regions\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Apply masking\n",
    "    perturbed_image = image.copy()\n",
    "    if mask_type == 'black':\n",
    "        perturbed_image[mask] = 0  # Set important regions to black\n",
    "    elif mask_type == 'mean':\n",
    "        mean_pixel_value = np.mean(image, axis=(0, 1), keepdims=True)\n",
    "        perturbed_image[mask] = mean_pixel_value  # Set important regions to mean pixel value\n",
    "    elif mask_type == 'blur':\n",
    "        blurred = cv2.GaussianBlur(image, (21, 21), 0)\n",
    "        perturbed_image[mask] = blurred[mask]  # Apply blur to important regions\n",
    "    \n",
    "    pil_image = Image.fromarray(perturbed_image.astype('uint8'))\n",
    "    return pil_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fcbecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_image = apply_mask(img, diff)\n",
    "\n",
    "plt.imshow(perturbed_image)\n",
    "\n",
    "\n",
    "\n",
    "perturbed_image_t = get_input_tensors(perturbed_image)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb07aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_mask(mask, shift_y, shift_x):\n",
    "    \"\"\"\n",
    "    Shifts the mask by given offsets. Wraps around if necessary.\n",
    "    \n",
    "    Args:\n",
    "        mask: Input binary mask (2D array).\n",
    "        shift_y: Vertical shift (positive for down, negative for up).\n",
    "        shift_x: Horizontal shift (positive for right, negative for left).\n",
    "    \n",
    "    Returns:\n",
    "        Shifted mask of the same size.\n",
    "    \"\"\"\n",
    "    # Perform shift with wrapping using np.roll\n",
    "    shifted_mask = np.roll(mask, shift_y, axis=0)  # Shift vertically\n",
    "    shifted_mask = np.roll(shifted_mask, shift_x, axis=1)  # Shift horizontally\n",
    "    return shifted_mask\n",
    "\n",
    "\n",
    "def shift_mask_new(mask, shift_y, shift_x):\n",
    "    \"\"\"\n",
    "    Shifts the mask by given offsets, ensuring no part of the mask extends outside the image boundaries.\n",
    "    \n",
    "    Args:\n",
    "        mask: Input binary mask (2D array).\n",
    "        shift_y: Vertical shift (positive for down, negative for up).\n",
    "        shift_x: Horizontal shift (positive for right, negative for left).\n",
    "    \n",
    "    Returns:\n",
    "        Shifted mask of the same size.\n",
    "    \"\"\"\n",
    "    h, w = mask.shape\n",
    "    shifted_mask = np.zeros_like(mask)  # Initialize the output mask with zeros\n",
    "\n",
    "    # Compute the valid region for the source and destination\n",
    "    src_y_start = max(0, shift_y)\n",
    "    src_y_end = min(h, h + shift_y)\n",
    "    dest_y_start = max(0, -shift_y)\n",
    "    dest_y_end = min(h, h - shift_y)\n",
    "\n",
    "    src_x_start = max(0, shift_x)\n",
    "    src_x_end = min(w, w + shift_x)\n",
    "    dest_x_start = max(0, -shift_x)\n",
    "    dest_x_end = min(w, w - shift_x)\n",
    "\n",
    "    # Place the mask in the new position without extending\n",
    "    shifted_mask[dest_y_start:dest_y_end, dest_x_start:dest_x_end] = mask[src_y_start:src_y_end, src_x_start:src_x_end]\n",
    "\n",
    "    return shifted_mask\n",
    "\n",
    "\n",
    "def apply_mask_random_new(image, heatmap, mask_type='black', threshold=np.percentile(diff, 80)):\n",
    "    \"\"\"\n",
    "    Mask the image based on the Grad-CAM heatmap.\n",
    "    Arguments:\n",
    "    - image: Original input image (numpy array)\n",
    "    - heatmap: Grad-CAM heatmap (numpy array)\n",
    "    - mask_type: Type of masking ('black', 'blur', or 'mean')\n",
    "    \"\"\"\n",
    "     \n",
    "    #T_R = transforms.Resize((112, 112))\n",
    "    #image = T_R(image)   \n",
    "    # Convert the PIL image to a numpy array\n",
    "    image = np.array(image)\n",
    "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))  # Resize heatmap to image size\n",
    "    mask = heatmap > threshold  # Create a binary mask of important regions\n",
    "    \n",
    "    # Generate random shifts\n",
    "    np.random.seed(42)\n",
    "    #shift_y = np.random.randint(-mask.shape[0] // 2, mask.shape[0] // 2)\n",
    "    #shift_x = np.random.randint(-mask.shape[1] // 2, mask.shape[1] // 2)\n",
    "    \n",
    "    shift_y = np.random.randint(-image.shape[0], image.shape[0])\n",
    "    shift_x = np.random.randint(-image.shape[1], image.shape[1])\n",
    "    \n",
    "\n",
    "    # Shift the mask\n",
    "    shifted_mask = shift_mask(mask, shift_y, shift_x)\n",
    "    \n",
    "    # Apply masking\n",
    "    perturbed_image = image.copy()\n",
    "    if mask_type == 'black':\n",
    "        perturbed_image[shifted_mask] = 0  # Set important regions to black\n",
    "    elif mask_type == 'mean':\n",
    "        mean_pixel_value = np.mean(image, axis=(0, 1), keepdims=True)\n",
    "        perturbed_image[mask] = mean_pixel_value  # Set important regions to mean pixel value\n",
    "    elif mask_type == 'blur':\n",
    "        blurred = cv2.GaussianBlur(image, (21, 21), 0)\n",
    "        perturbed_image[mask] = blurred[mask]  # Apply blur to important regions\n",
    "    \n",
    "    pil_image = Image.fromarray(perturbed_image.astype('uint8'))\n",
    "    return pil_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e182193",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image = apply_mask_random_new(img, diff)\n",
    "plt.imshow(random_image)\n",
    "\n",
    "random_image_t = get_input_tensors(random_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab5e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = probs5.indices[0, 0].item() \n",
    "print(target_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff334812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_confidence_drop(model, device, original_image, perturbed_image, target_class):\n",
    "    \"\"\"\n",
    "    Measure the drop in confidence between the original and perturbed image.\n",
    "    Arguments:\n",
    "    - model: Trained model\n",
    "    - original_image: Original input image (tensor)\n",
    "    - perturbed_image: Image with masked important regions (tensor)\n",
    "    - target_class: Target class for the prediction\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Original confidence\n",
    "    original_output = model(original_image.cuda())\n",
    "    original_confidence = torch.softmax(original_output, dim=1)[0, target_class].item()\n",
    "\n",
    "    # Perturbed confidence\n",
    "    perturbed_output = model(perturbed_image.cuda())\n",
    "    perturbed_confidence = torch.softmax(perturbed_output, dim=1)[0, target_class].item()\n",
    "\n",
    "    # Calculate confidence drop\n",
    "    confidence_drop = original_confidence - perturbed_confidence\n",
    "    return confidence_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f262ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SDD_confidence_drop = evaluate_confidence_drop(model, device, img_t, perturbed_image_t, target_class)\n",
    "print(SDD_confidence_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_confidence_drop = evaluate_confidence_drop(model, device, img_t, random_image_t, target_class)\n",
    "print(random_confidence_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cf53d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_mask_sdd(image, heatmap, threshold=np.percentile(diff, 80)):\n",
    "    \"\"\"\n",
    "    Mask the image based on the Grad-CAM heatmap.\n",
    "    Arguments:\n",
    "    - image: Original input image (numpy array)\n",
    "    - heatmap: Grad-CAM heatmap (numpy array)\n",
    "    - mask_type: Type of masking ('black', 'blur', or 'mean')\n",
    "    \"\"\"\n",
    "    # Convert to NumPy array\n",
    "    image = np.array(image)\n",
    "\n",
    "    # Resize heatmap to match the image size\n",
    "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
    "\n",
    "    # Create a binary mask\n",
    "    mask = heatmap > threshold\n",
    "\n",
    "    # Ensure the image is a proper copy\n",
    "    perturbed_image = np.copy(image)\n",
    "\n",
    "    # Initialize the result image with black\n",
    "    result = np.zeros_like(perturbed_image)\n",
    "\n",
    "  \n",
    "\n",
    "    # Copy the mask region to the result\n",
    "    result[mask] = perturbed_image[mask]\n",
    "    \n",
    "    \n",
    "    pil_image = Image.fromarray(result.astype('uint8'))\n",
    "    return pil_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b314f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdd_image = only_mask_sdd(img, diff)\n",
    "\n",
    "plt.imshow(sdd_image)\n",
    "\n",
    "\n",
    "\n",
    "sdd_image_t = get_input_tensors(sdd_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c1dda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_mask_random(image, heatmap, threshold=np.percentile(diff, 80)):\n",
    "    \"\"\"\n",
    "    Mask the image based on the Grad-CAM heatmap.\n",
    "    Arguments:\n",
    "    - image: Original input image (numpy array)\n",
    "    - heatmap: Grad-CAM heatmap (numpy array)\n",
    "    - mask_type: Type of masking ('black', 'blur', or 'mean')\n",
    "    \"\"\"\n",
    "     \n",
    "   \n",
    "    # Convert the PIL image to a numpy array\n",
    "    image = np.array(image)\n",
    "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))  # Resize heatmap to image size\n",
    "    mask = heatmap > threshold  # Create a binary mask of important regions\n",
    "    \n",
    "    # Generate random shifts\n",
    "    np.random.seed(42)\n",
    "    #shift_y = np.random.randint(-mask.shape[0] // 2, mask.shape[0] // 2)\n",
    "    #shift_x = np.random.randint(-mask.shape[1] // 2, mask.shape[1] // 2)\n",
    "    \n",
    "    shift_y = np.random.randint(-image.shape[0], image.shape[0])\n",
    "    shift_x = np.random.randint(-image.shape[1], image.shape[1])\n",
    "    \n",
    "\n",
    "    # Shift the mask\n",
    "    shifted_mask = shift_mask(mask, shift_y, shift_x)\n",
    "    \n",
    "     # Ensure the image is a proper copy\n",
    "    perturbed_image = np.copy(image)\n",
    "\n",
    "    # Initialize the result image with black\n",
    "    result = np.zeros_like(perturbed_image)\n",
    "\n",
    "  \n",
    "\n",
    "    # Copy the mask region to the result\n",
    "    result[shifted_mask] = perturbed_image[shifted_mask]\n",
    "    \n",
    "    \n",
    "    pil_image = Image.fromarray(result.astype('uint8'))\n",
    "    return pil_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1685afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_mask_image = only_mask_random(img, diff)\n",
    "\n",
    "plt.imshow(random_mask_image)\n",
    "\n",
    "\n",
    "\n",
    "random_mask_image_t = get_input_tensors(random_mask_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b1b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "SDD_mask_confidence_drop = evaluate_confidence_drop(model, device, img_t, sdd_image_t, target_class)\n",
    "print(SDD_mask_confidence_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a9a110",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_mask_confidence_drop = evaluate_confidence_drop(model, device, img_t, random_mask_image_t, target_class)\n",
    "print(random_mask_confidence_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6477e49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

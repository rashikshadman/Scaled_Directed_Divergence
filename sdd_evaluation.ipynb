{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72cae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os, json\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from flashtorch.utils import apply_transforms, load_image \n",
    "import net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d07248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_transform(pil_image):\n",
    "    \"\"\"Custom transform incorporating face alignment and preprocessing.\"\"\"\n",
    "    #aligned_rgb_img = align.get_aligned_face(pil_image)\n",
    "    np_img = np.array(pil_image)\n",
    "    bgr_img = ((np_img[:, :, ::-1] / 255.0) - 0.5) / 0.5  # Normalize to [-1, 1]\n",
    "    tensor = torch.tensor(bgr_img.transpose(2, 0, 1)).float()\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b423968",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_input_tensors(img):\n",
    "    \n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.Lambda(lambda img: custom_transform(img)),\n",
    "    # Add more transformations if needed (e.g., data augmentation)\n",
    "    ])\n",
    "    \n",
    "    # unsqeeze converts single image to batch of 1\n",
    "    return transform(img).unsqueeze(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0030292f",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_input_tensors_no_resize(img):\n",
    "    \n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "    #transforms.Resize((112, 112)),\n",
    "    transforms.Lambda(lambda img: custom_transform(img)),\n",
    "    # Add more transformations if needed (e.g., data augmentation)\n",
    "    ])\n",
    "    \n",
    "    # unsqeeze converts single image to batch of 1\n",
    "    return transform(img).unsqueeze(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb527e",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_input_tensors_single(img):\n",
    "    \n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "    #transforms.Resize((112, 112)),\n",
    "    transforms.Lambda(lambda img: custom_transform(img)),\n",
    "    # Add more transformations if needed (e.g., data augmentation)\n",
    "    ])\n",
    "    \n",
    "    # unsqeeze converts single image to batch of 1\n",
    "    return transform(img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d6520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the path to pretrained AdaFace models\n",
    "adaface_models = {\n",
    "    'ir_101': \"models/adaface_ir101_webface12m.ckpt\",\n",
    "}\n",
    "\n",
    "def load_pretrained_model(architecture='ir_101'):\n",
    "    \"\"\"Load the pretrained AdaFace model.\"\"\"\n",
    "    assert architecture in adaface_models.keys(), f\"Architecture {architecture} not supported.\"\n",
    "    model = net.build_model(architecture)\n",
    "    statedict = torch.load(adaface_models[architecture])['state_dict']\n",
    "    # Remove 'model.' prefix from keys\n",
    "    model_statedict = {key[6:]: val for key, val in statedict.items() if key.startswith('model.')}\n",
    "    model.load_state_dict(model_statedict)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    return model\n",
    "    \n",
    "    \n",
    "class FaceClassifier(nn.Module):\n",
    "    def __init__(self, feature_dim, num_classes, freeze_feature_extractor=True):\n",
    "        super(FaceClassifier, self).__init__()\n",
    "        self.feature_extractor = load_pretrained_model('ir_101')\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.classifier = nn.Linear(self.feature_dim, self.num_classes)\n",
    "\n",
    "        if freeze_feature_extractor:\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = False  # Freeze feature extractor\n",
    "        #else:\n",
    "             # Unfreeze the feature extractor\n",
    "            #for param in self.feature_extractor.parameters():\n",
    "                #param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features, _ = self.feature_extractor(x)\n",
    "        #print(type(features))\n",
    "        out = self.classifier(features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87bb311",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import io \n",
    "\n",
    "import scipy.ndimage as nd \n",
    "\n",
    "from flashtorch.saliency import Backprop \n",
    "from flashtorch.activmax import GradientAscent \n",
    "\n",
    "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0779088",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hook():\n",
    "    def __init__(self, module, backward=False):\n",
    "        if backward==False:\n",
    "            self.hook = module.register_forward_hook(self.hook_fn)\n",
    "        else:\n",
    "            self.hook = module.register_backward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "    def close(self):\n",
    "        self.hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bffa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_cam(model, output, activations, class_index):\n",
    "    # remove batch and transpose to 7,7,2048\n",
    "    out_features = activations.output.squeeze(0)\n",
    "    out_features = np.transpose(out_features.cpu().detach(), (1,2,0))\n",
    "    fw = model.classifier.weight[class_index,:]\n",
    "    cam = np.dot(out_features.detach(), fw.detach().cpu())\n",
    "    return cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7066e1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_cam(model, input_tensor, target_class, activations):\n",
    "    \"\"\"\n",
    "    Generate Score-CAM for a specific class.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The classification model.\n",
    "        input_tensor (torch.Tensor): Input image tensor of shape (1, C, H, W).\n",
    "        target_class (int): The target class index.\n",
    "        activations: Hook object containing feature maps from the target layer.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Score-CAM heatmap.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Get activation maps from hook (shape: [C, H, W])\n",
    "        act = activations.output.squeeze(0)  # Remove batch dim\n",
    "        num_channels, h, w = act.shape\n",
    "\n",
    "        # Normalize activation maps to [0, 1]\n",
    "        min_vals = torch.amin(act, dim=(1, 2), keepdim=True)\n",
    "        max_vals = torch.amax(act, dim=(1, 2), keepdim=True)\n",
    "        act = (act - min_vals) / (max_vals - min_vals + 1e-8)\n",
    "\n",
    "        # Resize each activation map to input size (H, W)\n",
    "        _, _, H, W = input_tensor.shape\n",
    "        upsampled_acts = F.interpolate(act.unsqueeze(0), size=(H, W), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        weights = []\n",
    "        for i in range(num_channels):\n",
    "            masked_input = input_tensor * upsampled_acts[0, i:i+1, :, :]\n",
    "            output = model(masked_input)\n",
    "            score = F.softmax(output, dim=1)[0, target_class].item()\n",
    "            weights.append(score)\n",
    "        \n",
    "        weights = np.array(weights).reshape(-1, 1, 1)\n",
    "        cam = (act.cpu().numpy() * weights).sum(axis=0)\n",
    "\n",
    "\n",
    "        cam = np.maximum(cam, 0)  # ReLU\n",
    "        #cam = cam - np.min(cam)\n",
    "        #cam = cam / (np.max(cam) + 1e-8)  # Normalize to [0, 1]\n",
    "        cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam) + 1e-8)\n",
    "        return cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d756bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sub_plots_op(original, activations, pclasses):\n",
    "    n = len(activations)\n",
    "    f, axarr = plt.subplots(1, n, figsize=(n * 5, 5))\n",
    "    \n",
    "    # Reverse the custom transformation to display the original image properly\n",
    "    inp = torch.squeeze(original, 0).numpy().transpose((1, 2, 0))  # Change to (H, W, C)\n",
    "    inp = ((inp * 0.5) + 0.5) * 255  # De-normalize from [-1, 1] to [0, 255]\n",
    "    inp = inp[:, :, ::-1]  # Convert back from BGR to RGB\n",
    "    inp = np.clip(inp / 255.0, 0, 1)  # Normalize to [0, 1] for display\n",
    "    \n",
    "    # Display each activation map with the original image as background\n",
    "    for i, cam in enumerate(activations):\n",
    "        axarr[i].axis('off')\n",
    "        axarr[i].imshow(inp, cmap='jet', alpha=1)\n",
    "        axarr[i].set_title(pclasses[i])\n",
    "        \n",
    "        # Calculate zoom factor based on the original and cam dimensions\n",
    "        orig_h, orig_w = inp.shape[:2]\n",
    "        cam_h, cam_w = cam.shape\n",
    "        zoom_factor = (orig_h / cam_h, orig_w / cam_w)\n",
    "        \n",
    "        # Rescale the activation map\n",
    "        zcam = nd.zoom(cam, zoom=zoom_factor, order=1)\n",
    "        axarr[i].imshow(zcam, cmap='jet', alpha=0.4)\n",
    "    \n",
    "    # Add a short pause for the plot to render\n",
    "    plt.pause(0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce22243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['subject_1', 'subject_10', 'subject_100', 'subject_101', 'subject_102', 'subject_103', 'subject_104', 'subject_105', 'subject_106', 'subject_107', 'subject_108', 'subject_109', 'subject_11', 'subject_110', 'subject_111', 'subject_112', 'subject_113', 'subject_114', 'subject_115', 'subject_116', 'subject_117', 'subject_118', 'subject_119', 'subject_12', 'subject_120', 'subject_121', 'subject_122', 'subject_123', 'subject_124', 'subject_125', 'subject_126', 'subject_127', 'subject_128', 'subject_129', 'subject_13', 'subject_130', 'subject_131', 'subject_132', 'subject_133', 'subject_134', 'subject_135', 'subject_136', 'subject_137', 'subject_138', 'subject_139', 'subject_14', 'subject_140', 'subject_141', 'subject_142', 'subject_143', 'subject_144', 'subject_145', 'subject_146', 'subject_147', 'subject_148', 'subject_149', 'subject_15', 'subject_150', 'subject_151', 'subject_152', 'subject_153', 'subject_154', 'subject_155', 'subject_156', 'subject_157', 'subject_158', 'subject_159', 'subject_16', 'subject_160', 'subject_161', 'subject_162', 'subject_163', 'subject_164', 'subject_165', 'subject_166', 'subject_167', 'subject_168', 'subject_169', 'subject_17', 'subject_170', 'subject_171', 'subject_172', 'subject_173', 'subject_174', 'subject_175', 'subject_176', 'subject_177', 'subject_178', 'subject_179', 'subject_18', 'subject_180', 'subject_181', 'subject_182', 'subject_183', 'subject_184', 'subject_185', 'subject_186', 'subject_187', 'subject_188', 'subject_189', 'subject_19', 'subject_190', 'subject_191', 'subject_192', 'subject_193', 'subject_194', 'subject_195', 'subject_196', 'subject_197', 'subject_198', 'subject_199', 'subject_2', 'subject_20', 'subject_200', 'subject_201', 'subject_202', 'subject_203', 'subject_204', 'subject_205', 'subject_206', 'subject_207', 'subject_208', 'subject_209', 'subject_21', 'subject_210', 'subject_211', 'subject_212', 'subject_213', 'subject_214', 'subject_215', 'subject_216', 'subject_217', 'subject_218', 'subject_219', 'subject_22', 'subject_220', 'subject_221', 'subject_222', 'subject_223', 'subject_224', 'subject_225', 'subject_226', 'subject_227', 'subject_228', 'subject_229', 'subject_23', 'subject_230', 'subject_231', 'subject_232', 'subject_233', 'subject_234', 'subject_235', 'subject_236', 'subject_237', 'subject_238', 'subject_239', 'subject_24', 'subject_240', 'subject_241', 'subject_242', 'subject_243', 'subject_244', 'subject_245', 'subject_246', 'subject_247', 'subject_248', 'subject_249', 'subject_25', 'subject_250', 'subject_251', 'subject_252', 'subject_253', 'subject_254', 'subject_255', 'subject_256', 'subject_257', 'subject_258', 'subject_259', 'subject_26', 'subject_260', 'subject_261', 'subject_262', 'subject_263', 'subject_264', 'subject_265', 'subject_266', 'subject_267', 'subject_268', 'subject_269', 'subject_27', 'subject_270', 'subject_271', 'subject_272', 'subject_273', 'subject_274', 'subject_275', 'subject_276', 'subject_277', 'subject_278', 'subject_279', 'subject_28', 'subject_280', 'subject_281', 'subject_282', 'subject_283', 'subject_284', 'subject_285', 'subject_286', 'subject_287', 'subject_288', 'subject_289', 'subject_29', 'subject_290', 'subject_291', 'subject_292', 'subject_293', 'subject_294', 'subject_295', 'subject_296', 'subject_297', 'subject_298', 'subject_299', 'subject_3', 'subject_30', 'subject_300', 'subject_301', 'subject_302', 'subject_303', 'subject_304', 'subject_305', 'subject_306', 'subject_307', 'subject_308', 'subject_309', 'subject_31', 'subject_310', 'subject_311', 'subject_312', 'subject_313', 'subject_314', 'subject_315', 'subject_316', 'subject_317', 'subject_318', 'subject_319', 'subject_32', 'subject_320', 'subject_321', 'subject_322', 'subject_323', 'subject_324', 'subject_325', 'subject_326', 'subject_327', 'subject_328', 'subject_329', 'subject_33', 'subject_330', 'subject_331', 'subject_332', 'subject_333', 'subject_334', 'subject_335', 'subject_336', 'subject_337', 'subject_338', 'subject_339', 'subject_34', 'subject_340', 'subject_341', 'subject_342', 'subject_343', 'subject_344', 'subject_345', 'subject_346', 'subject_347', 'subject_348', 'subject_349', 'subject_35', 'subject_350', 'subject_351', 'subject_352', 'subject_353', 'subject_354', 'subject_355', 'subject_356', 'subject_357', 'subject_358', 'subject_359', 'subject_36', 'subject_360', 'subject_361', 'subject_362', 'subject_363', 'subject_364', 'subject_365', 'subject_366', 'subject_367', 'subject_368', 'subject_369', 'subject_37', 'subject_370', 'subject_371', 'subject_372', 'subject_373', 'subject_374', 'subject_375', 'subject_376', 'subject_377', 'subject_378', 'subject_379', 'subject_38', 'subject_380', 'subject_381', 'subject_382', 'subject_383', 'subject_384', 'subject_385', 'subject_386', 'subject_387', 'subject_388', 'subject_389', 'subject_39', 'subject_390', 'subject_391', 'subject_392', 'subject_393', 'subject_394', 'subject_395', 'subject_396', 'subject_397', 'subject_398', 'subject_399', 'subject_4', 'subject_40', 'subject_400', 'subject_401', 'subject_402', 'subject_403', 'subject_404', 'subject_405', 'subject_406', 'subject_407', 'subject_408', 'subject_409', 'subject_41', 'subject_410', 'subject_411', 'subject_412', 'subject_413', 'subject_414', 'subject_415', 'subject_416', 'subject_417', 'subject_418', 'subject_419', 'subject_42', 'subject_420', 'subject_421', 'subject_422', 'subject_423', 'subject_424', 'subject_425', 'subject_426', 'subject_427', 'subject_428', 'subject_429', 'subject_43', 'subject_430', 'subject_431', 'subject_432', 'subject_433', 'subject_434', 'subject_435', 'subject_436', 'subject_437', 'subject_438', 'subject_439', 'subject_44', 'subject_440', 'subject_441', 'subject_442', 'subject_443', 'subject_444', 'subject_445', 'subject_446', 'subject_447', 'subject_448', 'subject_449', 'subject_45', 'subject_450', 'subject_451', 'subject_452', 'subject_453', 'subject_454', 'subject_455', 'subject_456', 'subject_457', 'subject_458', 'subject_459', 'subject_46', 'subject_460', 'subject_461', 'subject_462', 'subject_463', 'subject_464', 'subject_465', 'subject_466', 'subject_467', 'subject_468', 'subject_469', 'subject_47', 'subject_470', 'subject_471', 'subject_472', 'subject_473', 'subject_474', 'subject_475', 'subject_476', 'subject_477', 'subject_478', 'subject_479', 'subject_48', 'subject_480', 'subject_481', 'subject_482', 'subject_483', 'subject_484', 'subject_485', 'subject_486', 'subject_487', 'subject_488', 'subject_489', 'subject_49', 'subject_490', 'subject_491', 'subject_492', 'subject_493', 'subject_494', 'subject_495', 'subject_496', 'subject_497', 'subject_498', 'subject_499', 'subject_5', 'subject_50', 'subject_500', 'subject_501', 'subject_502', 'subject_503', 'subject_504', 'subject_505', 'subject_506', 'subject_507', 'subject_508', 'subject_509', 'subject_51', 'subject_510', 'subject_511', 'subject_512', 'subject_513', 'subject_514', 'subject_515', 'subject_516', 'subject_517', 'subject_518', 'subject_519', 'subject_52', 'subject_520', 'subject_521', 'subject_522', 'subject_523', 'subject_524', 'subject_525', 'subject_526', 'subject_527', 'subject_528', 'subject_529', 'subject_53', 'subject_530', 'subject_54', 'subject_55', 'subject_56', 'subject_57', 'subject_58', 'subject_59', 'subject_6', 'subject_60', 'subject_61', 'subject_62', 'subject_63', 'subject_64', 'subject_65', 'subject_66', 'subject_67', 'subject_68', 'subject_69', 'subject_7', 'subject_70', 'subject_71', 'subject_72', 'subject_73', 'subject_74', 'subject_75', 'subject_76', 'subject_77', 'subject_78', 'subject_79', 'subject_8', 'subject_80', 'subject_81', 'subject_82', 'subject_83', 'subject_84', 'subject_85', 'subject_86', 'subject_87', 'subject_88', 'subject_89', 'subject_9', 'subject_90', 'subject_91', 'subject_92', 'subject_93', 'subject_94', 'subject_95', 'subject_96', 'subject_97', 'subject_98', 'subject_99']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c71eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def apply_mask(image, heatmap, mask_type='black', acc=True):\n",
    "    \"\"\"\n",
    "    Mask the image based on the Grad-CAM heatmap.\n",
    "    Arguments:\n",
    "    - image: Original input image (numpy array)\n",
    "    - heatmap: Grad-CAM heatmap (numpy array)\n",
    "    - mask_type: Type of masking ('black', 'blur', or 'mean')\n",
    "    \"\"\"\n",
    "    if acc:\n",
    "       T_R = transforms.Resize((112, 112))\n",
    "       image = T_R(image)\n",
    "    threshold = np.percentile(heatmap, 80)\n",
    "     # Convert the PIL image to a numpy array\n",
    "    image = np.array(image)\n",
    "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))  # Resize heatmap to image size\n",
    "    mask = heatmap > threshold  # Create a binary mask of important regions\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Apply masking\n",
    "    perturbed_image = image.copy()\n",
    "    if mask_type == 'black':\n",
    "        perturbed_image[mask] = 0  # Set important regions to black\n",
    "    elif mask_type == 'mean':\n",
    "        mean_pixel_value = np.mean(image, axis=(0, 1), keepdims=True)\n",
    "        perturbed_image[mask] = mean_pixel_value  # Set important regions to mean pixel value\n",
    "    elif mask_type == 'blur':\n",
    "        blurred = cv2.GaussianBlur(image, (21, 21), 0)\n",
    "        perturbed_image[mask] = blurred[mask]  # Apply blur to important regions\n",
    "    \n",
    "    pil_image = Image.fromarray(perturbed_image.astype('uint8'))\n",
    "    return pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79121db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_mask(mask, shift_y, shift_x):\n",
    "    \"\"\"\n",
    "    Shifts the mask by given offsets. Wraps around if necessary.\n",
    "    \n",
    "    Args:\n",
    "        mask: Input binary mask (2D array).\n",
    "        shift_y: Vertical shift (positive for down, negative for up).\n",
    "        shift_x: Horizontal shift (positive for right, negative for left).\n",
    "    \n",
    "    Returns:\n",
    "        Shifted mask of the same size.\n",
    "    \"\"\"\n",
    "    # Perform shift with wrapping using np.roll\n",
    "    shifted_mask = np.roll(mask, shift_y, axis=0)  # Shift vertically\n",
    "    shifted_mask = np.roll(shifted_mask, shift_x, axis=1)  # Shift horizontally\n",
    "    return shifted_mask\n",
    "\n",
    "\n",
    "\n",
    "def apply_mask_random(image, heatmap, mask_type='black', acc=True):\n",
    "     \n",
    "    if acc:\n",
    "      T_R = transforms.Resize((112, 112))\n",
    "      image = T_R(image)\n",
    "    threshold = np.percentile(heatmap, 80)   \n",
    "    # Convert the PIL image to a numpy array\n",
    "    image = np.array(image)\n",
    "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))  # Resize heatmap to image size\n",
    "    mask = heatmap > threshold  # Create a binary mask of important regions\n",
    "    \n",
    "    # Generate random shifts\n",
    "    np.random.seed(42)\n",
    "    #shift_y = np.random.randint(-mask.shape[0] // 2, mask.shape[0] // 2)\n",
    "    #shift_x = np.random.randint(-mask.shape[1] // 2, mask.shape[1] // 2)\n",
    "    \n",
    "    shift_y = np.random.randint(-image.shape[0], image.shape[0])\n",
    "    shift_x = np.random.randint(-image.shape[1], image.shape[1])\n",
    "    \n",
    "\n",
    "    # Shift the mask\n",
    "    shifted_mask = shift_mask(mask, shift_y, shift_x)\n",
    "    \n",
    "    # Apply masking\n",
    "    perturbed_image = image.copy()\n",
    "    if mask_type == 'black':\n",
    "        perturbed_image[shifted_mask] = 0  # Set important regions to black\n",
    "    elif mask_type == 'mean':\n",
    "        mean_pixel_value = np.mean(image, axis=(0, 1), keepdims=True)\n",
    "        perturbed_image[mask] = mean_pixel_value  # Set important regions to mean pixel value\n",
    "    elif mask_type == 'blur':\n",
    "        blurred = cv2.GaussianBlur(image, (21, 21), 0)\n",
    "        perturbed_image[mask] = blurred[mask]  # Apply blur to important regions\n",
    "    \n",
    "    pil_image = Image.fromarray(perturbed_image.astype('uint8'))\n",
    "    return pil_image\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_confidence_drop(model, device, original_image, perturbed_image, target_class):\n",
    "    \"\"\"\n",
    "    Measure the drop in confidence between the original and perturbed image.\n",
    "    Arguments:\n",
    "    - model: Trained model\n",
    "    - original_image: Original input image (tensor)\n",
    "    - perturbed_image: Image with masked important regions (tensor)\n",
    "    - target_class: Target class for the prediction\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Original confidence\n",
    "    original_output = model(original_image.cuda())\n",
    "    original_confidence = F.softmax(original_output, dim=1)[0, target_class].item()\n",
    "\n",
    "    # Perturbed confidence\n",
    "    perturbed_output = model(perturbed_image.cuda())\n",
    "    perturbed_confidence = F.softmax(perturbed_output, dim=1)[0, target_class].item()\n",
    "\n",
    "    # Calculate confidence drop\n",
    "    confidence_drop = original_confidence - perturbed_confidence\n",
    "    return confidence_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b6f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, original_image, perturbed_image, random_perturbed_image):\n",
    "\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "\n",
    "  # Original confidence\n",
    "  original_output = model(original_image.cuda())\n",
    "  original_probs = F.softmax(original_output, dim=1)\n",
    "  original_top_prob, original_top_class = torch.max(original_probs, dim=1)\n",
    "  original_top_prob = original_top_prob.item()\n",
    "  original_top_class = original_top_class.item()\n",
    "\n",
    "\n",
    "\n",
    "  # Perturbed confidence\n",
    "  perturbed_output = model(perturbed_image.cuda())\n",
    "  perturbed_probs = F.softmax(perturbed_output, dim=1)\n",
    "  perturbed_top_prob, perturbed_top_class = torch.max(perturbed_probs, dim=1)\n",
    "  perturbed_top_prob = perturbed_top_prob.item()\n",
    "  perturbed_top_class = perturbed_top_class.item()\n",
    "  \n",
    "  perturbed_confidence = F.softmax(perturbed_output, dim=1)[0, original_top_class].item()\n",
    "\n",
    "  \n",
    "  # random Perturbed confidence\n",
    "  random_perturbed_output = model(random_perturbed_image.cuda())\n",
    "  random_perturbed_probs = F.softmax(random_perturbed_output, dim=1)\n",
    "  random_perturbed_top_prob, random_perturbed_top_class = torch.max(random_perturbed_probs, dim=1)\n",
    "  random_perturbed_top_prob = random_perturbed_top_prob.item()\n",
    "  random_perturbed_top_class = random_perturbed_top_class.item()\n",
    "  \n",
    "  random_perturbed_confidence = F.softmax(random_perturbed_output, dim=1)[0, original_top_class].item()\n",
    "\n",
    "  return original_top_class, original_top_prob, perturbed_top_class, perturbed_top_prob, perturbed_confidence, random_perturbed_top_class, random_perturbed_top_prob, random_perturbed_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf8351",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def SDD_Generation(image_path):\n",
    "\n",
    "# check for cuda and set gpu or cpu\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "  \n",
    "  img = load_image(image_path)\n",
    "\n",
    "\n",
    "  SDD_Model = 'models/facescrub_adamW_webface.pt'\n",
    "  model = torch.load(SDD_Model)\n",
    "  model.to(device)\n",
    "\n",
    "  img_t = get_input_tensors(img)\n",
    "  img_t_ = img_t.cuda()\n",
    "  model.eval()\n",
    "\n",
    "  logits = model(img_t.cuda())\n",
    "  \n",
    "  probs = F.softmax(logits, dim=1)\n",
    "  probs5 = probs.topk(5)\n",
    "\n",
    "  target_class = probs5.indices[0, 0].item() \n",
    "  \n",
    "  #SDD_Model = 'Desktop/AdaFace/models/facescrub_adamW_webface.pt'\n",
    "\n",
    "  #model = torch.load(SDD_Model)\n",
    " \n",
    "  #model.to(device)\n",
    "  #model.eval()\n",
    "     \n",
    "\n",
    "  hook = Hook(model.feature_extractor.body[48].res_layer[4])\n",
    "  output = model(img_t.cuda())\n",
    "\n",
    "  cams = []\n",
    "  pclasses = []\n",
    "\n",
    "  # Iterate over the top 5 classes and generate CAM for each\n",
    "  for i in range(5):\n",
    "      class_index = probs5.indices[0, i].item()  # Get the class index for the top-i prediction\n",
    "      cam = get_score_cam(model, img_t_, class_index, hook)\n",
    "      #cam = get_class_cam(model, output, hook, class_index=class_index)  # Generate CAM for this class\n",
    "      cams.append(cam)\n",
    "    \n",
    "     # Append the corresponding class name from 'class_names'\n",
    "      pclasses.append(class_names[class_index])\n",
    "\n",
    "  # Now 'cams' contains the CAMs for the top 5 classes,\n",
    "  # and 'pclasses' contains the corresponding subject names.\n",
    "  scale = 5 * (np.mean([np.abs(cam).mean() for cam in cams[1:]]) / np.abs(cams[0]).mean())\n",
    "  scale = np.clip(scale, 5, 50)\n",
    "\n",
    "  cams[0] = cams[0] * scale\n",
    "  diff = cams[0] - cams[1]\n",
    "  diff = diff - cams[2]\n",
    "  diff = diff - cams[3]\n",
    "  diff = diff - cams[4]\n",
    "\n",
    "\n",
    "  # Compute dynamic exponential factor\n",
    "  std_dev = np.std(diff)\n",
    "  factor = 0.2 + (std_dev / (std_dev + 1)) * 0.2  # Adaptive factor based on spread \n",
    "\n",
    "\n",
    "\n",
    "  # Apply exponential\n",
    "  diff = np.exp(factor * diff)\n",
    "  #cams.append(diff)\n",
    "\n",
    "  #pclasses.append(\"SDD_\" + pclasses[0])\n",
    "\n",
    "#original = img_t.detach().cpu()\n",
    "#show_sub_plots_op(original, cams, pclasses)\n",
    "#plt.show()\n",
    "    \n",
    "#pclass = \"SDD_CAM\"\n",
    "    \n",
    "#show_single_cam_op(original, diff, pclass)\n",
    "#plt.show\n",
    "  return img, diff, target_class, model, device, img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb372f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdd_generate_data(image_path):\n",
    "  \n",
    "  img, diff, t_c, model, device, img_t = SDD_Generation(image_path)\n",
    "    \n",
    "  perturbed_image = apply_mask(img, diff, mask_type='black', acc=False)\n",
    "\n",
    "  perturbed_image_t = get_input_tensors(perturbed_image)\n",
    "    \n",
    "  random_image = apply_mask_random(img, diff, mask_type='black', acc=False)\n",
    "\n",
    "  random_image_t = get_input_tensors(random_image)\n",
    "\n",
    "    \n",
    "  #target_class = probs5.indices[0, 0].item() \n",
    "   \n",
    "  #SDD_confidence_drop = evaluate_confidence_drop(model, img_t, perturbed_image_t, target_class)\n",
    "   \n",
    "  #random_confidence_drop = evaluate_confidence_drop(model, img_t, random_image_t, target_class)\n",
    "  original_top_class, original_top_prob, perturbed_top_class, perturbed_top_prob, perturbed_confidence, random_perturbed_top_class, random_perturbed_top_prob, random_perturbed_confidence = evaluate(model, device, img_t, perturbed_image_t, random_image_t) \n",
    "  \n",
    "  return  original_top_class, original_top_prob, perturbed_top_class, perturbed_top_prob, perturbed_confidence, random_perturbed_top_class, random_perturbed_top_prob, random_perturbed_confidence\n",
    "\n",
    "\n",
    "\n",
    "#SDD, random = sdd_generate(image_path='Desktop/AdaFace/facescrub/data_facescrub_sdd/Orlando_Bloom_88510_45958.jpeg')\n",
    "\n",
    "#print(SDD)\n",
    "                           \n",
    "#print(random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b532c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdd_generate_evaluate(image_path):\n",
    "    \n",
    " \n",
    "  img, diff, target_class, model, device, img_t = SDD_Generation(image_path)\n",
    " \n",
    "    \n",
    "  perturbed_image = apply_mask(img, diff, mask_type='black', acc=False)\n",
    "\n",
    "  perturbed_image_t = get_input_tensors(perturbed_image)\n",
    "    \n",
    "  random_image = apply_mask_random(img, diff, mask_type='black', acc=False)\n",
    "\n",
    "  random_image_t = get_input_tensors(random_image)\n",
    "\n",
    "    \n",
    "  \n",
    "   \n",
    "  SDD_confidence_drop = evaluate_confidence_drop(model, device, img_t, perturbed_image_t, target_class)\n",
    "   \n",
    "  random_confidence_drop = evaluate_confidence_drop(model, device, img_t, random_image_t, target_class)\n",
    "  \n",
    "\n",
    "  return SDD_confidence_drop, random_confidence_drop\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2270e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_mask_sdd(image, heatmap):\n",
    "    \"\"\"\n",
    "    Mask the image based on the Grad-CAM heatmap.\n",
    "    Arguments:\n",
    "    - image: Original input image (numpy array)\n",
    "    - heatmap: Grad-CAM heatmap (numpy array)\n",
    "    - mask_type: Type of masking ('black', 'blur', or 'mean')\n",
    "    \"\"\"\n",
    "    threshold = np.percentile(heatmap, 80)\n",
    "    # Convert to NumPy array\n",
    "    image = np.array(image)\n",
    "\n",
    "    # Resize heatmap to match the image size\n",
    "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
    "\n",
    "    # Create a binary mask\n",
    "    mask = heatmap > threshold\n",
    "\n",
    "    # Ensure the image is a proper copy\n",
    "    perturbed_image = np.copy(image)\n",
    "\n",
    "    # Initialize the result image with black\n",
    "    result = np.zeros_like(perturbed_image)\n",
    "\n",
    "  \n",
    "\n",
    "    # Copy the mask region to the result\n",
    "    result[mask] = perturbed_image[mask]\n",
    "    \n",
    "    \n",
    "    pil_image = Image.fromarray(result.astype('uint8'))\n",
    "    return pil_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2b8518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_mask_random(image, heatmap):\n",
    "    \"\"\"\n",
    "    Mask the image based on the Grad-CAM heatmap.\n",
    "    Arguments:\n",
    "    - image: Original input image (numpy array)\n",
    "    - heatmap: Grad-CAM heatmap (numpy array)\n",
    "    - mask_type: Type of masking ('black', 'blur', or 'mean')\n",
    "    \"\"\"\n",
    "     \n",
    "    threshold = np.percentile(heatmap, 80)\n",
    "    # Convert the PIL image to a numpy array\n",
    "    image = np.array(image)\n",
    "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))  # Resize heatmap to image size\n",
    "    mask = heatmap > threshold  # Create a binary mask of important regions\n",
    "    \n",
    "    # Generate random shifts\n",
    "    np.random.seed(42)\n",
    "    #shift_y = np.random.randint(-mask.shape[0] // 2, mask.shape[0] // 2)\n",
    "    #shift_x = np.random.randint(-mask.shape[1] // 2, mask.shape[1] // 2)\n",
    "    \n",
    "    shift_y = np.random.randint(-image.shape[0], image.shape[0])\n",
    "    shift_x = np.random.randint(-image.shape[1], image.shape[1])\n",
    "    \n",
    "\n",
    "    # Shift the mask\n",
    "    shifted_mask = shift_mask(mask, shift_y, shift_x)\n",
    "    \n",
    "     # Ensure the image is a proper copy\n",
    "    perturbed_image = np.copy(image)\n",
    "\n",
    "    # Initialize the result image with black\n",
    "    result = np.zeros_like(perturbed_image)\n",
    "\n",
    "  \n",
    "\n",
    "    # Copy the mask region to the result\n",
    "    result[shifted_mask] = perturbed_image[shifted_mask]\n",
    "    \n",
    "    \n",
    "    pil_image = Image.fromarray(result.astype('uint8'))\n",
    "    return pil_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee6905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdd_mask_evaluate(image_path):\n",
    "    \n",
    " \n",
    "  img, diff, target_class, model, device, img_t = SDD_Generation(image_path)\n",
    " \n",
    "    \n",
    "  perturbed_image = only_mask_sdd(img, diff)\n",
    "\n",
    "  perturbed_image_t = get_input_tensors(perturbed_image)\n",
    "    \n",
    "  random_image = only_mask_random(img, diff)\n",
    "\n",
    "  random_image_t = get_input_tensors(random_image)\n",
    "\n",
    "    \n",
    "  \n",
    "   \n",
    "  SDD_confidence_drop = evaluate_confidence_drop(model, device, img_t, perturbed_image_t, target_class)\n",
    "   \n",
    "  random_confidence_drop = evaluate_confidence_drop(model, device, img_t, random_image_t, target_class)\n",
    "  \n",
    "\n",
    "  return SDD_confidence_drop, random_confidence_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74c68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdd_mask_data(image_path):\n",
    "    \n",
    "  img, diff, t_c, model, device, img_t = SDD_Generation(image_path)\n",
    "    \n",
    "  perturbed_image = only_mask_sdd(img, diff)\n",
    "\n",
    "  perturbed_image_t = get_input_tensors(perturbed_image)\n",
    "    \n",
    "  random_image = only_mask_random(img, diff)\n",
    "\n",
    "  random_image_t = get_input_tensors(random_image)\n",
    "\n",
    "    \n",
    "  #target_class = probs5.indices[0, 0].item() \n",
    "   \n",
    "  #SDD_confidence_drop = evaluate_confidence_drop(model, img_t, perturbed_image_t, target_class)\n",
    "   \n",
    "  #random_confidence_drop = evaluate_confidence_drop(model, img_t, random_image_t, target_class)\n",
    "  original_top_class, original_top_prob, perturbed_top_class, perturbed_top_prob, perturbed_confidence, random_perturbed_top_class, random_perturbed_top_prob, random_perturbed_confidence = evaluate(model, device, img_t, perturbed_image_t, random_image_t) \n",
    "  \n",
    "  return  original_top_class, original_top_prob, perturbed_top_class, perturbed_top_prob, perturbed_confidence, random_perturbed_top_class, random_perturbed_top_prob, random_perturbed_confidence\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcdeba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the folder containing subfolders of images\n",
    "root_folder = \"Data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e0d4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47f15a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_folder_one(folder_path):\n",
    "    \"\"\"\n",
    "    Process all images in a folder and its subfolders, calculate average confidence drops.\n",
    "    \"\"\"\n",
    "    sdd_confidence_drop_avg = []\n",
    "    random_confidence_drop_avg = []\n",
    "  \n",
    "    \n",
    "\n",
    "    for subdir, _, files in os.walk(folder_path):\n",
    "       for file in files:  # Loop through each file in the current directory\n",
    "            image_path = os.path.join(subdir, file)\n",
    "            drop_case1, drop_case2 = sdd_generate_evaluate(image_path)\n",
    "            #drop_case = sdd_generate_evaluate(image_path)\n",
    "            sdd_confidence_drop_avg.append(drop_case1)\n",
    "            random_confidence_drop_avg.append(drop_case2)\n",
    "          \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    avg_drop_sdd = np.mean(sdd_confidence_drop_avg)\n",
    "    avg_drop_random = np.mean(random_confidence_drop_avg)\n",
    "\n",
    "    return avg_drop_sdd, avg_drop_random\n",
    "\n",
    "   \n",
    "        \n",
    "        \n",
    "# Process the folder and calculate average confidence drops\n",
    "avg_case1, avg_case2 = process_folder_one(root_folder)\n",
    "#avg_case1 = process_folder_one(root_folder)\n",
    "\n",
    "print(f\"Average Confidence Drop (SDD): {avg_case1}\")\n",
    "print(f\"Average Confidence Drop (random): {avg_case2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c36cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_folder_mask(folder_path):\n",
    "    \"\"\"\n",
    "    Process all images in a folder and its subfolders, calculate average confidence drops.\n",
    "    \"\"\"\n",
    "    sdd_confidence_drop_avg = []\n",
    "    random_confidence_drop_avg = []\n",
    "  \n",
    "    \n",
    "\n",
    "    for subdir, _, files in os.walk(folder_path):\n",
    "       for file in files:  # Loop through each file in the current directory\n",
    "            image_path = os.path.join(subdir, file)\n",
    "            drop_case1, drop_case2 = sdd_mask_evaluate(image_path)\n",
    "            sdd_confidence_drop_avg.append(drop_case1)\n",
    "            random_confidence_drop_avg.append(drop_case2)\n",
    "          \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    avg_drop_sdd = np.mean(sdd_confidence_drop_avg)\n",
    "    avg_drop_random = np.mean(random_confidence_drop_avg)\n",
    "\n",
    "    return avg_drop_sdd, avg_drop_random\n",
    "\n",
    "   \n",
    "        \n",
    "        \n",
    "# Process the folder and calculate average confidence drops\n",
    "avg_case_mask1, avg_case_mask2 = process_folder_mask(root_folder)\n",
    "\n",
    "print(f\"Average Confidence Drop (SDD): {avg_case_mask1}\")\n",
    "print(f\"Average Confidence Drop (random): {avg_case_mask2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b090b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "def process_folder_two(folder_path):\n",
    "    \"\"\"\n",
    "    Process all images in a folder and its subfolders, calculate average confidence drops.\n",
    "    \"\"\"\n",
    "    #sdd_confidence_drop_avg = []\n",
    "    #random_confidence_drop_avg = []\n",
    "    image_name = []\n",
    "    original_top_class_all = []\n",
    "    original_top_prob_all = []\n",
    "    perturbed_top_class_all = []\n",
    "    perturbed_top_prob_all = []\n",
    "    perturbed_confidence_all = []\n",
    "    random_perturbed_top_class_all = []\n",
    "    random_perturbed_top_prob_all = []\n",
    "    random_perturbed_confidence_all = []\n",
    "\n",
    "    for subdir, _, files in os.walk(folder_path):\n",
    "       for file in files:  # Loop through each file in the current directory\n",
    "            image_name.append(file)\n",
    "            image_path = os.path.join(subdir, file)\n",
    "            #drop_case1, drop_case2 = sdd_generate(image_path)\n",
    "            #sdd_confidence_drop_avg.append(drop_case1)\n",
    "            #random_confidence_drop_avg.append(drop_case2)\n",
    "            original_top_class, original_top_prob, perturbed_top_class, perturbed_top_prob, perturbed_confidence, random_perturbed_top_class, random_perturbed_top_prob, random_perturbed_confidence = sdd_generate_data(image_path) \n",
    "            original_top_class_all.append(original_top_class)\n",
    "            original_top_prob_all.append(original_top_prob)\n",
    "            perturbed_top_class_all.append(perturbed_top_class)\n",
    "            perturbed_top_prob_all.append(perturbed_top_prob)\n",
    "            perturbed_confidence_all.append(perturbed_confidence)\n",
    "            random_perturbed_top_class_all.append(random_perturbed_top_class)\n",
    "            random_perturbed_top_prob_all.append(random_perturbed_top_prob)\n",
    "            random_perturbed_confidence_all.append(random_perturbed_confidence)\n",
    "            \n",
    "            \n",
    "            \n",
    "  \n",
    "\n",
    "    # Combine lists into a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "           'Image': image_name,\n",
    "           'Original_Prediction': original_top_class_all,\n",
    "           'Original_Prob': original_top_prob_all,\n",
    "           'SDD_Pertb_Prediction': perturbed_top_class_all,\n",
    "           'SDD_Pertb_Prob': perturbed_top_prob_all,\n",
    "           'SDD_Pertb_Prob_Original_Pred': perturbed_confidence_all,\n",
    "           'Random_Pertb_Prediction': random_perturbed_top_class_all,\n",
    "           'Random_Pertb_Prob': random_perturbed_top_prob_all,\n",
    "           'Random_Pertb_Prob_Original_Pred': random_perturbed_confidence_all\n",
    "                         })\n",
    "\n",
    "    print(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df = process_folder_two(root_folder)\n",
    "        \n",
    "\n",
    "end = time.time()\n",
    "print(f\"Runtime: {end - start:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ebf82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder_mask_two(folder_path):\n",
    "    \"\"\"\n",
    "    Process all images in a folder and its subfolders, calculate average confidence drops.\n",
    "    \"\"\"\n",
    "    #sdd_confidence_drop_avg = []\n",
    "    #random_confidence_drop_avg = []\n",
    "    image_name = []\n",
    "    original_top_class_all = []\n",
    "    original_top_prob_all = []\n",
    "    perturbed_top_class_all = []\n",
    "    perturbed_top_prob_all = []\n",
    "    perturbed_confidence_all = []\n",
    "    random_perturbed_top_class_all = []\n",
    "    random_perturbed_top_prob_all = []\n",
    "    random_perturbed_confidence_all = []\n",
    "\n",
    "    for subdir, _, files in os.walk(folder_path):\n",
    "       for file in files:  # Loop through each file in the current directory\n",
    "            image_name.append(file)\n",
    "            image_path = os.path.join(subdir, file)\n",
    "            #drop_case1, drop_case2 = sdd_generate(image_path)\n",
    "            #sdd_confidence_drop_avg.append(drop_case1)\n",
    "            #random_confidence_drop_avg.append(drop_case2)\n",
    "            original_top_class, original_top_prob, perturbed_top_class, perturbed_top_prob, perturbed_confidence, random_perturbed_top_class, random_perturbed_top_prob, random_perturbed_confidence = sdd_mask_data(image_path) \n",
    "            original_top_class_all.append(original_top_class)\n",
    "            original_top_prob_all.append(original_top_prob)\n",
    "            perturbed_top_class_all.append(perturbed_top_class)\n",
    "            perturbed_top_prob_all.append(perturbed_top_prob)\n",
    "            perturbed_confidence_all.append(perturbed_confidence)\n",
    "            random_perturbed_top_class_all.append(random_perturbed_top_class)\n",
    "            random_perturbed_top_prob_all.append(random_perturbed_top_prob)\n",
    "            random_perturbed_confidence_all.append(random_perturbed_confidence)\n",
    "            \n",
    "            \n",
    "            \n",
    "  \n",
    "\n",
    "    # Combine lists into a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "           'Image': image_name,\n",
    "           'Original_Prediction': original_top_class_all,\n",
    "           'Original_Prob': original_top_prob_all,\n",
    "           'SDD_Pertb_Prediction': perturbed_top_class_all,\n",
    "           'SDD_Pertb_Prob': perturbed_top_prob_all,\n",
    "           'SDD_Pertb_Prob_Original_Pred': perturbed_confidence_all,\n",
    "           'Random_Pertb_Prediction': random_perturbed_top_class_all,\n",
    "           'Random_Pertb_Prob': random_perturbed_top_prob_all,\n",
    "           'Random_Pertb_Prob_Original_Pred': random_perturbed_confidence_all\n",
    "                         })\n",
    "\n",
    "    print(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df_2 = process_folder_mask_two(root_folder)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca290ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Count the number of differences\n",
    "num_differences_1 = (df_2['Original_Prediction'] != df_2['SDD_Pertb_Prediction']).sum()\n",
    "\n",
    "# Calculate the percentage of differences\n",
    "percentage_differences_1 = (num_differences_1 / len(df_2['Original_Prediction'])) * 100\n",
    "\n",
    "print(f\"Percentage of differences for SDD: {percentage_differences_1:.2f}%\")\n",
    "\n",
    "\n",
    "# Count the number of differences\n",
    "num_differences_2 = (df_2['Original_Prediction'] != df_2['Random_Pertb_Prediction']).sum()\n",
    "\n",
    "# Calculate the percentage of differences\n",
    "percentage_differences_2 = (num_differences_2 / len(df_2['Original_Prediction'])) * 100\n",
    "\n",
    "print(f\"Percentage of differences for Random: {percentage_differences_2:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eff8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract Column2 from Column1\n",
    "difference_1 = df_2['Original_Prob'] - df_2['SDD_Pertb_Prob_Original_Pred']\n",
    "difference_2 = df_2['Original_Prob'] - df_2['Random_Pertb_Prob_Original_Pred']\n",
    "\n",
    "# Calculate the average of the difference\n",
    "average_difference_1 = difference_1.mean()\n",
    "average_difference_2 = difference_2.mean()\n",
    "\n",
    "print(\"SDD Confidence Drop:\", average_difference_1)\n",
    "print(\"Random Confidence Drop:\", average_difference_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc900148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 5))\n",
    "\n",
    "df_2['Original_Prob'].hist(bins=10, ax=axes[0])\n",
    "axes[0].set_title('Original_Probability')\n",
    "axes[0].set_xlabel('Probability')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "\n",
    "df_2['SDD_Pertb_Prob_Original_Pred'].hist(bins=10, ax=axes[1])\n",
    "axes[1].set_title('SDD_Retention_Prob_Original_Pred')\n",
    "axes[1].set_xlabel('Probability')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "\n",
    "df_2['Random_Pertb_Prob_Original_Pred'].hist(bins=10, ax=axes[2])\n",
    "axes[2].set_title('Random_Retention_Prob_Original_Pred')\n",
    "axes[2].set_xlabel('Probability')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919cca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 5))\n",
    "\n",
    "# List of column names and titles\n",
    "columns = ['Original_Prob', 'SDD_Pertb_Prob_Original_Pred', 'Random_Pertb_Prob_Original_Pred']\n",
    "titles = ['Original Probability', 'SDD Retention Prob Original Pred', 'Random Retention Prob Original Pred']\n",
    "\n",
    "for i, col in enumerate(columns):\n",
    "    # Compute mean and standard deviation\n",
    "    mean, std = df_2[col].mean(), df_2[col].std()\n",
    "\n",
    "    # Generate x values for the bell curve\n",
    "    x = np.linspace(df_2[col].min(), df_2[col].max(), 100)\n",
    "    y = stats.norm.pdf(x, mean, std)\n",
    "\n",
    "    # Plot bell curve\n",
    "    axes[i].plot(x, y, 'r-', lw=2)\n",
    "\n",
    "    # Set labels\n",
    "    axes[i].set_title(titles[i])\n",
    "    axes[i].set_xlabel('Probability')\n",
    "    axes[i].set_ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a8f3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
